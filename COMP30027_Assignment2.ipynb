{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#importing neccesary utils and libraries, to optimize the train time, please trained all models \n",
    "# in this notebook using GPU\n",
    "\n",
    "import os \n",
    "from os.path import join\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, MaxPooling2D, Conv2D, BatchNormalization, Activation\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from keras.utils import plot_model\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold, cross_val_score \n",
    "from sklearn.svm import SVC \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import  ConfusionMatrixDisplay, accuracy_score, recall_score, precision_score, f1_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "import time\n",
    "\n",
    "\n",
    "IMG_SIZE = 32\n",
    "INPUT_SHAPE = (32, 32, 3)\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCH  = 60 \n",
    "NUM_CLASSES = 43 \n",
    "NUM_FOLDS = 5\n",
    "\n",
    "## Replace the path to your germanzip file in this section \n",
    "## e.g. : train = 'path/germanzip/2025_A2/train\n",
    "\n",
    "train = '/kaggle/input/germanzip/2025_A2/train'\n",
    "test  = '/kaggle/input/germanzip/2025_A2/test'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Sections\n",
    "1. Data Analysis\n",
    "2. Data visualization and Augementation \n",
    "3. Support Vector Machine + Logisitc Regression \n",
    "4. Convolutional Neural Network\n",
    "5. Ensemble Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Label Overview\n",
    "classes = { 0:'Speed limit (20km/h)',\n",
    "            1:'Speed limit (30km/h)', \n",
    "            2:'Speed limit (50km/h)', \n",
    "            3:'Speed limit (60km/h)', \n",
    "            4:'Speed limit (70km/h)', \n",
    "            5:'Speed limit (80km/h)', \n",
    "            6:'End of speed limit (80km/h)', \n",
    "            7:'Speed limit (100km/h)', \n",
    "            8:'Speed limit (120km/h)', \n",
    "            9:'No passing', \n",
    "            10:'No passing veh over 3.5 tons', \n",
    "            11:'Right-of-way at intersection', \n",
    "            12:'Priority road', \n",
    "            13:'Yield', \n",
    "            14:'Stop', \n",
    "            15:'No vehicles', \n",
    "            16:'Veh > 3.5 tons prohibited', \n",
    "            17:'No entry', \n",
    "            18:'General caution', \n",
    "            19:'Dangerous curve left', \n",
    "            20:'Dangerous curve right', \n",
    "            21:'Double curve', \n",
    "            22:'Bumpy road', \n",
    "            23:'Slippery road', \n",
    "            24:'Road narrows on the right', \n",
    "            25:'Road work', \n",
    "            26:'Traffic signals', \n",
    "            27:'Pedestrians', \n",
    "            28:'Children crossing', \n",
    "            29:'Bicycles crossing', \n",
    "            30:'Beware of ice/snow',\n",
    "            31:'Wild animals crossing', \n",
    "            32:'End speed + passing limits', \n",
    "            33:'Turn right ahead', \n",
    "            34:'Turn left ahead', \n",
    "            35:'Ahead only', \n",
    "            36:'Go straight or right', \n",
    "            37:'Go straight or left', \n",
    "            38:'Keep right', \n",
    "            39:'Keep left', \n",
    "            40:'Roundabout mandatory', \n",
    "            41:'End of no passing', \n",
    "            42:'End no passing veh > 3.5 tons' }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load train and test metadata\n",
    "train_metadata = pd.read_csv(join(train, 'train_metadata.csv'))\n",
    "test_metadata  = pd.read_csv(join(test,  'test_metadata.csv'))\n",
    "train_metadata.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Analysis on the distribution and reasoning for adding data \n",
    "\n",
    "num_instance = [0] * NUM_CLASSES\n",
    "for _, row in tqdm(train_metadata.iterrows(), desc=\"Processing\"):\n",
    "    num_instance[row['ClassId']] += 1\n",
    "\n",
    "frequency = np.array(num_instance)\n",
    "labels = [classes[i] for i in range(NUM_CLASSES)]\n",
    "\n",
    "plt.figure(figsize=(22, 10))\n",
    "bars = plt.bar(range(NUM_CLASSES), frequency, tick_label=labels)\n",
    "plt.xlabel('Class Label')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Traffic Sign Class Distribution') \n",
    "plt.xticks(rotation=60)\n",
    "plt.savefig('Class_distribution.png')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Image visualization and Augementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_images, train_labels = [], []\n",
    "for i, row in tqdm(train_metadata.iterrows(), desc=f\"Processing\"):\n",
    "    img_path = os.path.join(train, row['image_path'])\n",
    "    image = cv2.imread(img_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (32, 32))  # Resize to 32x32\n",
    "    train_images.append(image)\n",
    "    train_labels.append(row['ClassId'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "train_images = np.array(train_images)\n",
    "train_labels = to_categorical(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Data Augmentation\n",
    "aug = ImageDataGenerator(\n",
    "    rescale = 1.0 / 255.0, \n",
    "    rotation_range = 10, \n",
    "    zoom_range=0.15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.15,\n",
    "    horizontal_flip=False,\n",
    "    vertical_flip=False,\n",
    "    fill_mode=\"nearest\",\n",
    "    brightness_range = [0.95, 2.95]\n",
    ")\n",
    "# for gamma \n",
    "aug_1 = ImageDataGenerator(\n",
    "    rescale = 1.0 / 255.0, \n",
    "    rotation_range = 10, \n",
    "    zoom_range=0.15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.15,\n",
    "    horizontal_flip=False,\n",
    "    vertical_flip=False,\n",
    "    fill_mode=\"nearest\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "trainGenerator = aug.flow(train_images[3:4], train_labels[3:4], batch_size=32) \n",
    "plt.figure(figsize=(25, 25))\n",
    "for i in range(10):\n",
    "    plt.subplot(5, 5, i+1)\n",
    "    x_batch, _= next(trainGenerator)\n",
    "    image = x_batch[0]\n",
    "    plt.imshow(x_batch[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Train Images Shape:\", train_images.shape)\n",
    "print(\"Train Labels Shape:\", train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Extraction and simple classifier SVM, Logistic Regression, KNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Image processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from skimage.feature import hog\n",
    "def compute_hog(img):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    _, hog_image = hog(gray, visualize = True)\n",
    "    return hog_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_features, train_hog_features = [], [] \n",
    "\n",
    "for i, row in tqdm(train_metadata.iterrows(), desc=f\"Processing\"):\n",
    "    img_path = os.path.join(train, row['image_path'])\n",
    "    image = cv2.imread(img_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (32, 32))  # Resize to 32x32\n",
    "    train_features.append(image)\n",
    "    train_hog_features.append(compute_hog(image))\n",
    "\n",
    "train_features     = np.array(train_features)\n",
    "train_hog_features = np.array(train_hog_features)\n",
    "\n",
    "train_features.resize(len(train_features), 3072)\n",
    "train_hog_features.resize(len(train_features), 1024)\n",
    "train_combine      = np.concatenate([train_features, train_hog_features], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_features, test_hog_features = [], []\n",
    "for i, row in tqdm(test_metadata.iterrows(),desc = f\"Processing\"): \n",
    "    img_path = os.path.join(test, row['image_path'])\n",
    "    image = cv2.imread(img_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (32, 32))\n",
    "    test_features.append(image)\n",
    "    test_hog_features.append(compute_hog(image))\n",
    "\n",
    "test_features = np.array(test_features)\n",
    "test_hog_features = np.array(test_hog_features)\n",
    "\n",
    "test_features.resize(len(test_features), 3072)\n",
    "test_hog_features.resize(len(test_features), 1024)\n",
    "test_combine     = np.concatenate([test_features, test_hog_features], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_class = []\n",
    "for i, row in tqdm(train_metadata.iterrows(), desc=f\"Processing\"):\n",
    "    train_class.append(row['ClassId'])\n",
    "\n",
    "train_class = np.array(train_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Evaluation using K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluation_on_simple_full(train_attr_name, train_attrs, model_name, model): \n",
    "  kf = KFold(n_splits = NUM_FOLDS, shuffle = True, random_state = 42)\n",
    "  print(\"=\" * 80)  \n",
    "  result = {'accuracy'  : 0.0, 'precision' : 0.0, 'recall' : 0.0, 'f1_score' : 0.0} \n",
    "  for train_index, validation_index in kf.split(train_attrs): \n",
    "       train_image, train_label = train_attrs[train_index], train_class[train_index]\n",
    "       validation_image, validation_label = train_attrs[validation_index], train_class[validation_index]\n",
    "       train_image = preprocessing.scale(train_image)\n",
    "       validation_image = preprocessing.scale(validation_image)\n",
    "       model.fit(train_image, train_label) \n",
    "       predictions = model.predict(validation_image)\n",
    "       ground_truth = validation_label\n",
    "      \n",
    "       result['accuracy']  += accuracy_score(ground_truth, predictions)\n",
    "       result['precision'] += precision_score(ground_truth, predictions, average = 'weighted')\n",
    "       result['recall']    += recall_score(ground_truth, predictions,    average = 'weighted')\n",
    "       result['f1_score']  += f1_score(ground_truth, predictions,        average = 'weighted')\n",
    "\n",
    "  result['accuracy']  /= NUM_FOLDS\n",
    "  result['precision'] /= NUM_FOLDS\n",
    "  result['recall']    /= NUM_FOLDS\n",
    "  result['f1_score']  /= NUM_FOLDS\n",
    "  print(f\"Result of training on {train_attr_name} using {model_name} with full features\")\n",
    "  print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for dataset, name in [(train_combine, \"Original + HOG\"),(train_features, \"Original\"), (train_hog_features, \"HOG\")]:\n",
    "    evaluation_on_simple_full(name, \n",
    "                              dataset,   \n",
    "                              \"Logistic Regression\", \n",
    "                               LogisticRegression(max_iter=200, multi_class='multinomial', solver='lbfgs', C = NUM_CLASSES))\n",
    "    evaluation_on_simple_full(name,  \n",
    "                              dataset, \n",
    "                              \"SVM\", \n",
    "                              SVC(kernel = 'rbf', C = NUM_CLASSES))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# this may take a while to train \n",
    "def evaluation_on_simple(train_attr_name, train_attrs, model_name, model, num): \n",
    "    kf = KFold(n_splits = NUM_FOLDS, shuffle = True, random_state = 42)\n",
    "    selectors = {\n",
    "      \"PCA\" : PCA(n_components = num),\n",
    "      \"Mutual Info\" : SelectKBest(score_func=mutual_info_classif, k = num)\n",
    "    }  \n",
    "    for selector_name, selector in selectors.items(): \n",
    "      print(\"=\" * 80)  \n",
    "      result = {'accuracy'  : 0.0, 'precision' : 0.0, 'recall' : 0.0, 'f1_score' : 0.0} \n",
    "      for train_index, validation_index in kf.split(train_attrs): \n",
    "           train_image, train_label = train_attrs[train_index], train_class[train_index]\n",
    "           validation_image, validation_label = train_attrs[validation_index], train_class[validation_index]\n",
    "\n",
    "           train_image = preprocessing.scale(train_image)\n",
    "           validation_image = preprocessing.scale(validation_image)\n",
    "          \n",
    "           train_image = selector.fit_transform(train_image, train_label)\n",
    "           validation_image = selector.transform(validation_image)\n",
    "           model.fit(train_image, train_label) \n",
    "           predictions = model.predict(validation_image)\n",
    "           ground_truth = validation_label\n",
    "           result['accuracy']  += accuracy_score(ground_truth, predictions)\n",
    "           result['precision'] += precision_score(ground_truth, predictions, average = 'weighted')\n",
    "           result['recall']    += recall_score(ground_truth, predictions,    average = 'weighted')\n",
    "           result['f1_score']  += f1_score(ground_truth, predictions,        average = 'weighted')\n",
    "    \n",
    "      result['accuracy']  /= NUM_FOLDS\n",
    "      result['precision'] /= NUM_FOLDS\n",
    "      result['recall']    /= NUM_FOLDS\n",
    "      result['f1_score']  /= NUM_FOLDS\n",
    "      print(f\"Result of training on {train_attr_name} using {model_name} with {selector_name}\")\n",
    "      print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for dataset, name in [(train_combine, \"Original + HOG\"), (train_features, \"Original\"), (train_hog_features, \"HOG\")]:\n",
    "    evaluation_on_simple(name, \n",
    "                         dataset,   \n",
    "                         \"Logistic Regression\", \n",
    "                         LogisticRegression(max_iter=200, multi_class='multinomial', solver='lbfgs', C = NUM_CLASSES),\n",
    "                         300)\n",
    "    evaluation_on_simple(name,  \n",
    "                         dataset, \n",
    "                         \"SVM\", \n",
    "                         SVC(kernel = 'rbf', C = NUM_CLASSES), \n",
    "                         300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Critical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Evaluation \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import  confusion_matrix, ConfusionMatrixDisplay, accuracy_score, classification_report\n",
    "\n",
    "svm = SVC(kernel = 'rbf', C = NUM_CLASSES)\n",
    "selector = SelectKBest(score_func=mutual_info_classif, k = 300)\n",
    "\n",
    "train_size = int(len(train_combine) * 0.8)\n",
    "X_train, y_train = train_combine[0 : train_size], train_class[0 : train_size]\n",
    "X_test,  y_test  = train_combine[train_size:],    train_class[train_size:]\n",
    "X_train = preprocessing.scale(X_train)\n",
    "X_test  = preprocessing.scale(X_test)\n",
    "x_train_features = selector.fit_transform(X_train, y_train) \n",
    "x_test_features  = selector.transform(X_test)\n",
    "svm.fit(x_train_features, y_train)\n",
    "y_pred = svm.predict(x_test_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"accuracy (SVM):\", accuracy_score(y_pred, y_test))\n",
    "cf = confusion_matrix(y_pred, y_test)\n",
    "df_cm = pd.DataFrame(cf, index = classes,  columns = classes)\n",
    "\n",
    "plt.figure(figsize = (20,20))\n",
    "sns.heatmap(df_cm, annot=True)\n",
    "plt.savefig('svm_mat.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "### See the error on the image \n",
    "from sklearn.metrics import  classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "wrong_pred = [] \n",
    "for i in range(len(y_test)): \n",
    "  if y_test[i] != y_pred[i] :\n",
    "    img_path = os.path.join(train, f\"img_00{train_size + i + 1}.jpg\")\n",
    "    image = cv2.imread(img_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (32, 32))  # Resize to 32x32\n",
    "    wrong_pred.append([image, y_pred[i], y_test[i]])\n",
    "\n",
    "plt.figure(figsize=(25, 25))\n",
    "for i, (image, pred, real) in enumerate(wrong_pred[:25]): \n",
    "    plt.subplot(5, 5, i + 1)\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')  # optional: turn off axis\n",
    "    plt.title(f\"Pred: {pred}, Real: {real}\")\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Export to test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import  confusion_matrix, ConfusionMatrixDisplay, accuracy_score, classification_report\n",
    "\n",
    "def get_result(model, model_name, num, train_data, test_data): \n",
    "    selector    =  SelectKBest(score_func=mutual_info_classif, k = num)\n",
    "    train_data = preprocessing.scale(train_data)\n",
    "    test_data  = preprocessing.scale(test_data)\n",
    "    train_image = selector.fit_transform(train_data, train_class)\n",
    "    test_image = selector.transform(test_data)\n",
    "    model.fit(train_image, train_class) \n",
    "    predictions = model.predict(test_image) \n",
    "    result = test_metadata.copy(deep = False)\n",
    "    result = result.drop(columns = ['image_path'])\n",
    "    for i, row in tqdm(result.iterrows(),desc = f\"Processing\"): \n",
    "        result.loc[i, 'ClassId'] = predictions[i] \n",
    "    result['ClassId'] = result['ClassId'].astype(int)   \n",
    "    result.to_csv(f'Naive_{model_name}.csv', index = False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "get_result(LogisticRegression(max_iter=200, multi_class='multinomial', solver='lbfgs', C = NUM_CLASSES), \n",
    "           \"LogisticRegression\", \n",
    "           300, \n",
    "           train_combine, \n",
    "           test_combine)\n",
    "get_result(SVC(kernel = 'rbf', C = NUM_CLASSES), \n",
    "           \"SVM\",\n",
    "           300, \n",
    "           train_combine, \n",
    "           test_combine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Convolutional Neuron Network Model (ALexNet Architecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def createAlexNet1(num_classes : int): \n",
    "  model = Sequential() \n",
    "  model.add(Conv2D(32, (3, 3), activation  = 'relu', input_shape = INPUT_SHAPE))\n",
    "  model.add(Conv2D(64, (3, 3), activation  = 'relu'))  \n",
    "  model.add(MaxPooling2D((2, 2)))    \n",
    "  model.add(Conv2D(128, (3, 3), activation  = 'relu'))    \n",
    "  model.add(Conv2D(256, (3, 3), activation  = 'relu'))\n",
    "  model.add(MaxPooling2D((2, 2))) \n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(512, activation = 'relu'))   \n",
    "  model.add(Dropout(0.5)) # Reduce overfitting \n",
    "  model.add(Dense(num_classes, activation = 'softmax'))\n",
    "  return model \n",
    "\n",
    "model = createAlexNet1(NUM_CLASSES)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#This model is based on AlexNet Architecture but adding batch normalization layer \n",
    "def createAlexNet2(num_classes : int): \n",
    "  model = Sequential() \n",
    "  model.add(Conv2D(32, (3, 3), activation  = 'relu', input_shape = INPUT_SHAPE))  \n",
    "  model.add(Conv2D(64, (3, 3), activation  = 'relu'))    \n",
    "  model.add(MaxPooling2D((2, 2)))\n",
    "  model.add(BatchNormalization(axis = -1))    \n",
    "  model.add(Conv2D(128, (3, 3), activation  = 'relu'))     \n",
    "  model.add(Conv2D(256, (3, 3), activation  = 'relu'))    \n",
    "  model.add(MaxPooling2D((2, 2)))    \n",
    "  model.add(BatchNormalization(axis = -1))    \n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(512, activation = 'relu'))\n",
    "  model.add(BatchNormalization())    \n",
    "  model.add(Dropout(0.5)) # Reduce overfitting \n",
    "  model.add(Dense(num_classes, activation = 'softmax'))\n",
    "  return model \n",
    "\n",
    "model = createAlexNet2(NUM_CLASSES)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Evaluation using K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    Plots training history including accuracy and loss.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    # Accuracy plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'])\n",
    "    \n",
    "    # Loss plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate_kfold(creator, data_augmented = True): \n",
    "    kf = KFold(n_splits = NUM_FOLDS, shuffle = True, random_state = 42)\n",
    "    \n",
    "    result = {'accuracy'  : 0.0, 'precision' : 0.0, 'recall' : 0.0, 'f1_score' : 0.0} \n",
    "    \n",
    "    for train_index, validation_index in kf.split(train_images): \n",
    "       train_image, train_label = train_images[train_index], train_labels[train_index]\n",
    "       validation_image, validation_label = train_images[validation_index], train_labels[validation_index] \n",
    "       cnn_model = creator(NUM_CLASSES)\n",
    "       cnn_model.compile(optimizer = 'adam', \n",
    "                         loss = 'categorical_crossentropy', \n",
    "                         metrics=['accuracy'])\n",
    "       if data_augmented: \n",
    "           trainGenerator = aug.flow(train_image, train_label, batch_size = BATCH_SIZE) \n",
    "           history = cnn_model.fit(trainGenerator, \n",
    "                                   steps_per_epoch = 100,\n",
    "                                   batch_size = BATCH_SIZE, \n",
    "                                   epochs = NUM_EPOCH,      \n",
    "                                   validation_data = (validation_image / 255.0, validation_label))\n",
    "       else: \n",
    "           history = cnn_model.fit(x = train_image,  \n",
    "                                   y = train_label, \n",
    "                                   batch_size = BATCH_SIZE, \n",
    "                                   epochs = NUM_EPOCH,      \n",
    "                                   validation_data = (validation_image / 255.0, validation_label)) \n",
    "        \n",
    "       predictions  = cnn_model.predict(validation_image / 255.0)\n",
    "       predictions  = np.argmax(predictions, axis = -1)\n",
    "       ground_truth = np.argmax(validation_label, axis = -1)\n",
    "       result['accuracy']  += accuracy_score(ground_truth, predictions)\n",
    "       result['precision'] += precision_score(ground_truth, predictions, average = 'weighted')\n",
    "       result['recall']    += recall_score(ground_truth, predictions,    average = 'weighted')\n",
    "       result['f1_score']  += f1_score(ground_truth, predictions,        average = 'weighted')\n",
    "    \n",
    "    result['accuracy']  /= NUM_FOLDS\n",
    "    result['precision'] /= NUM_FOLDS\n",
    "    result['recall']    /= NUM_FOLDS\n",
    "    result['f1_score']  /= NUM_FOLDS\n",
    "    return result\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "result = train_and_evaluate_kfold(createAlexNet1, False)\n",
    "result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "result = train_and_evaluate_kfold(createAlexNet2, False)\n",
    "result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "result = train_and_evaluate_kfold(createAlexNet1)\n",
    "result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "result = train_and_evaluate_kfold(createAlexNet2)\n",
    "result "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 - Export to test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def prediction_test(creator, num): \n",
    "    trainGenerator = aug.flow(train_images, train_labels, batch_size=BATCH_SIZE) \n",
    "    cnn_model = creator(NUM_CLASSES)\n",
    "    cnn_model.compile(optimizer = 'adam', \n",
    "                      loss   = 'categorical_crossentropy', \n",
    "                      metrics=['accuracy'])\n",
    "    history = cnn_model.fit(trainGenerator, \n",
    "                            batch_size = BATCH_SIZE, \n",
    "                            steps_per_epoch=100,  \n",
    "                            epochs = NUM_EPOCH)\n",
    "    test_images = [] \n",
    "    for i, row in tqdm(test_metadata.iterrows(),desc = f\"Processing\"): \n",
    "        img_path = os.path.join(test, row['image_path'])\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, (32, 32))\n",
    "        test_images.append(image)\n",
    "    \n",
    "    test_images = np.array(test_images)\n",
    "    predictions = cnn_model.predict(test_images / 255.0)\n",
    "    result = test_metadata.copy(deep = False)\n",
    "    result = result.drop(columns = ['image_path'])\n",
    "    for i, row in tqdm(result.iterrows(),desc = f\"Processing\"): \n",
    "        result.loc[i, 'ClassId'] = np.argmax(predictions[i]) \n",
    "    result['ClassId'] = result['ClassId'].astype(int)   \n",
    "    result.to_csv(f'new_resultAlexNet{num}.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "prediction_test(createAlexNet1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "prediction_test(createAlexNet2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. Critical Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def evaluation_on_hold_out(creator, num = 1, data_augmentation=True):\n",
    "    train_image,  validation_image, train_label, validation_label = train_test_split(train_images, train_labels, test_size = 0.2,random_state = 42)\n",
    "    cnn_model = creator(NUM_CLASSES)\n",
    "    cnn_model.compile(optimizer='adam', \n",
    "                      loss='categorical_crossentropy', \n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "    if data_augmentation:\n",
    "        trainGenerator = aug.flow(train_image, train_label, batch_size=BATCH_SIZE) \n",
    "        history = cnn_model.fit(trainGenerator, \n",
    "                                steps_per_epoch=100,\n",
    "                                epochs=NUM_EPOCH,      \n",
    "                                validation_data=(validation_image / 255.0, validation_label))\n",
    "    else:\n",
    "        history = cnn_model.fit(x=train_image / 255.0, \n",
    "                                y=train_label, \n",
    "                                batch_size=BATCH_SIZE, \n",
    "                                epochs=NUM_EPOCH,      \n",
    "                                validation_data=(validation_image / 255.0, validation_label)) \n",
    "\n",
    "    plot_training_history(history)\n",
    "\n",
    "    predictions = cnn_model.predict(validation_image / 255.0)\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "    ground_truth = np.argmax(validation_label, axis=-1)\n",
    "\n",
    "    misclassify_instance = [i for i in range(len(predictions)) if predictions[i] != ground_truth[i]]\n",
    "\n",
    "    # Confusion matrix\n",
    "    plt.figure(figsize = (15, 15))\n",
    "    cf = confusion_matrix(ground_truth, predictions)\n",
    "    df_cm = pd.DataFrame(cf, index=classes, columns=classes)\n",
    "    plt.figure(figsize = (20,20))\n",
    "    sns.heatmap(df_cm, annot=True)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.savefig(f\"confusion_matrix_{num}.png\")  \n",
    "    plt.show()\n",
    "    # Show misclassified images (up to 25)\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    wrong_img = [] \n",
    "    for i in range(min(25, len(misclassify_instance))): \n",
    "        index = misclassify_instance[i]\n",
    "        plt.subplot(5, 5, i + 1)\n",
    "        plt.imshow(validation_image[index])\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Pred: {predictions[index]}, Real: {ground_truth[index]}\")\n",
    "        wrong_img.append([validation_image[index], ground_truth[index]])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return wrong_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "wrong_img1 = evaluation_on_hold_out(createAlexNet1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "wrong_img2 = evaluation_on_hold_out(createAlexNet2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Ensemble Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Image transformation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def apply_filters(img):\n",
    "    bilateral = cv2.bilateralFilter(img, 9, 75, 75)\n",
    "    return bilateral\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def smoothing(image): \n",
    " kernel = np.ones((5 , 5), np.float32) / 25.0\n",
    " imgSmooth = cv2.filter2D(image,-1, kernel)\n",
    " return imgSmooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def adjust_gamma(image):\n",
    "    gamma = random.uniform(1.5, 2.0)\n",
    "    invGamma = 1.0 / gamma\n",
    "    table = np.array([((i / 255.0) ** invGamma) * 255 for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "    return cv2.LUT(image, table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "train_labels = []\n",
    "train_images, bilateral_images, smoothing_images, gamma_images = [], [], [], [] \n",
    "test_images, test_bilateral_images, test_smoothing_images, test_gamma_images = [], [], [], [] \n",
    "\n",
    "for i, row in tqdm(train_metadata.iterrows(), desc=f\"Processing\"):\n",
    "    img_path = os.path.join(train, row['image_path'])\n",
    "    image = cv2.imread(img_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    bilateral_image = apply_filters(image)\n",
    "    smooth_image = smoothing(image) \n",
    "    gamma_image  = adjust_gamma(image)\n",
    "    \n",
    "    image           = cv2.resize(image, (32, 32))\n",
    "    bilateral_image = cv2.resize(bilateral_image, (32, 32))\n",
    "    smooth_image    = cv2.resize(smooth_image, (32, 32))\n",
    "    gamma_image     = cv2.resize(gamma_image, (32, 32))\n",
    "\n",
    "    train_images.append(image)\n",
    "    bilateral_images.append(bilateral_image)\n",
    "    smoothing_images.append(smooth_image)\n",
    "    gamma_images.append(gamma_image)\n",
    "    train_labels.append(row['ClassId'])\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "train_images     = np.array(train_images)\n",
    "bilateral_images = np.array(bilateral_images)\n",
    "smoothing_images = np.array(smoothing_images)\n",
    "gamma_images     = np.array(gamma_images)\n",
    "train_labels = to_categorical(train_labels)\n",
    "\n",
    "for i, row in tqdm(test_metadata.iterrows(),desc = f\"Processing\"): \n",
    "    img_path = os.path.join(test, row['image_path'])\n",
    "    image = cv2.imread(img_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    bilateral_image = apply_filters(image)\n",
    "    smooth_image = smoothing(image)\n",
    "    gamma_image  = adjust_gamma(image)\n",
    "\n",
    "    image           = cv2.resize(image, (32, 32))\n",
    "    bilateral_image = cv2.resize(bilateral_image, (32, 32))\n",
    "    smooth_image    = cv2.resize(smooth_image, (32, 32))\n",
    "    gamma_image     = cv2.resize(gamma_image, (32, 32))\n",
    "    \n",
    "    test_images.append(image)\n",
    "    test_bilateral_images.append(bilateral_image)\n",
    "    test_smoothing_images.append(smooth_image)\n",
    "    test_gamma_images.append(gamma_image)\n",
    "\n",
    "test_images           = np.array(test_images)\n",
    "test_bilateral_images = np.array(test_bilateral_images) \n",
    "test_smoothing_images = np.array(test_smoothing_images) \n",
    "test_gamma_images     = np.array(test_gamma_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Evaluation using K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def ensemble_method(creator): \n",
    "    kf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=42)\n",
    "    result_log = {'accuracy'  : 0.0, 'precision' : 0.0, 'recall' : 0.0, 'f1_score' : 0.0} \n",
    "    result_svm = {'accuracy'  : 0.0, 'precision' : 0.0, 'recall' : 0.0, 'f1_score' : 0.0} \n",
    "    # Only use the selected image sets\n",
    "    image_sets = [\n",
    "        (train_images, aug),         \n",
    "        (bilateral_images, aug),\n",
    "        (gamma_images, aug_1),\n",
    "    ]  \n",
    "    for train_index, validation_index in kf.split(train_images): \n",
    "        train_label = train_labels[train_index]\n",
    "        validation_label = train_labels[validation_index]\n",
    "\n",
    "        base_train_result = [] \n",
    "        base_valid_result = [] \n",
    "        \n",
    "        for (image_set, aug_method) in image_sets: \n",
    "            train_input = image_set[train_index]\n",
    "            validation_input = image_set[validation_index]\n",
    "            \n",
    "            cnn_model = creator(NUM_CLASSES)\n",
    "            cnn_model.compile(optimizer='adam', \n",
    "                              loss='categorical_crossentropy', \n",
    "                              metrics=['accuracy'])\n",
    "            \n",
    "            trainGenerator = aug_method.flow(train_input, train_label, batch_size=BATCH_SIZE)\n",
    "            history = cnn_model.fit(trainGenerator,\n",
    "                                    steps_per_epoch=100,\n",
    "                                    epochs=NUM_EPOCH, \n",
    "                                    validation_data = (validation_input / 255.0, validation_label))\n",
    "            \n",
    "            base_train_result.append(cnn_model.predict(train_input      / 255.0))\n",
    "            base_valid_result.append(cnn_model.predict(validation_input / 255.0))\n",
    "        \n",
    "        # Combine outputs from base models\n",
    "        train_features = np.hstack(base_train_result)\n",
    "        valid_features = np.hstack(base_valid_result)\n",
    "        train_label_int = np.argmax(train_label, axis=1)\n",
    "        valid_label_int = np.argmax(validation_label, axis=1)\n",
    "        \n",
    "        ground_truth = valid_label_int\n",
    "\n",
    "        # Logistic Regression \n",
    "        log_reg = LogisticRegression(multi_class='multinomial', solver='lbfgs', C=NUM_CLASSES, max_iter=1000)\n",
    "        log_reg.fit(train_features, train_label_int)\n",
    "        predictions = log_reg.predict(valid_features)\n",
    "        print(\"Accuracy of logistic regression:\", accuracy_score(ground_truth, predictions))\n",
    "        result_log['accuracy']  += accuracy_score(ground_truth, predictions)\n",
    "        result_log['precision'] += precision_score(ground_truth, predictions, average = 'weighted')\n",
    "        result_log['recall']    += recall_score(ground_truth, predictions,    average = 'weighted')\n",
    "        result_log['f1_score']  += f1_score(ground_truth, predictions,        average = 'weighted')\n",
    "\n",
    "        # Support vector machine\n",
    "        \n",
    "        svm_model = SVC(kernel = 'rbf', C = NUM_CLASSES)\n",
    "        svm_model.fit(train_features, train_label_int)\n",
    "        predictions = svm_model.predict(valid_features)\n",
    "        print(\"Accuracy of SVM:\", accuracy_score(ground_truth, predictions))\n",
    "        result_svm['accuracy']  += accuracy_score(ground_truth, predictions)\n",
    "        result_svm['precision'] += precision_score(ground_truth, predictions, average = 'weighted')\n",
    "        result_svm['recall']    += recall_score(ground_truth, predictions,    average = 'weighted')\n",
    "        result_svm['f1_score']  += f1_score(ground_truth, predictions,        average = 'weighted')\n",
    "    result_log['accuracy']  /= NUM_FOLDS\n",
    "    result_log['precision'] /= NUM_FOLDS\n",
    "    result_log['recall']    /= NUM_FOLDS\n",
    "    result_log['f1_score']  /= NUM_FOLDS\n",
    "    result_svm['accuracy']  /= NUM_FOLDS\n",
    "    result_svm['precision'] /= NUM_FOLDS\n",
    "    result_svm['recall']    /= NUM_FOLDS\n",
    "    result_svm['f1_score']  /= NUM_FOLDS\n",
    "    return result_log, result_svm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "result_log, result_svm = ensemble_method(createAlexNet1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "result_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "result_svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Export to Output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "image_sets = [(train_images, test_images, aug), \n",
    "              (gamma_images, test_gamma_images, aug_1), \n",
    "              (bilateral_images, test_bilateral_images, aug)] \n",
    "\n",
    "def train_and_test(creator, num = 1):\n",
    "    base_train_result = [] \n",
    "    base_tests_result = [] \n",
    "    \n",
    "    for (train_input, tests_input, aug_method) in image_sets: \n",
    "        cnn_model = creator(NUM_CLASSES)\n",
    "        cnn_model.compile(optimizer='adam', \n",
    "                          loss='categorical_crossentropy', \n",
    "                          metrics=['accuracy'])\n",
    "        trainGenerator = aug_method.flow(train_input, train_labels, batch_size = BATCH_SIZE)\n",
    "        history = cnn_model.fit(trainGenerator,\n",
    "                                steps_per_epoch = 100,\n",
    "                                epochs = NUM_EPOCH)\n",
    "        \n",
    "        base_train_result.append(cnn_model.predict(train_input / 255.0))\n",
    "        base_tests_result.append(cnn_model.predict(tests_input / 255.0))\n",
    "    \n",
    "    # Combine outputs from base models\n",
    "    train_features = np.hstack(base_train_result)\n",
    "    tests_features = np.hstack(base_tests_result)\n",
    "    train_label_int = np.argmax(train_labels, axis=1)\n",
    "    \n",
    "    # Logistic Regression \n",
    "    log_reg = LogisticRegression(multi_class='multinomial', solver='lbfgs', C=NUM_CLASSES, max_iter=1000)\n",
    "    log_reg.fit(train_features, train_label_int)\n",
    "    predictions = log_reg.predict(tests_features)\n",
    "    result = test_metadata.copy(deep = False)\n",
    "    result = result.drop(columns = ['image_path'])\n",
    "    for i, row in tqdm(result.iterrows(),desc = f\"Processing\"): \n",
    "        result.loc[i, 'ClassId'] = predictions[i]\n",
    "    result['ClassId'] = result['ClassId'].astype(int)   \n",
    "    result.to_csv(f'result_ensemble_AlexNet{num}_with_log.csv', index = False)\n",
    "    # SVM \n",
    "    svm_model = SVC(kernel = 'rbf', C = NUM_CLASSES)\n",
    "    svm_model.fit(train_features, train_label_int)\n",
    "    predictions = svm_model.predict(tests_features)\n",
    "    result = test_metadata.copy(deep = False)\n",
    "    result = result.drop(columns = ['image_path'])\n",
    "    for i, row in tqdm(result.iterrows(),desc = f\"Processing\"): \n",
    "        result.loc[i, 'ClassId'] = predictions[i]\n",
    "    result['ClassId'] = result['ClassId'].astype(int)   \n",
    "    result.to_csv(f'result_ensemble_AlexNet{num}_with_svm.csv', index = False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_and_test(createAlexNet1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4. Critical Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def ensemble_method_evaluate(creator): \n",
    "    kf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "    # Only use the selected image sets\n",
    "    image_sets = [\n",
    "        (train_images, aug),        \n",
    "        (bilateral_images, aug), \n",
    "        (gamma_images, aug_1)\n",
    "    ]  \n",
    "    for train_index, validation_index in kf.split(train_images):\n",
    "        validation_image = train_images[validation_index]\n",
    "        train_label = train_labels[train_index]\n",
    "        validation_label = train_labels[validation_index]\n",
    "\n",
    "        base_train_result = [] \n",
    "        base_valid_result = [] \n",
    "        \n",
    "        for (image_set, aug_method) in image_sets: \n",
    "            train_input = image_set[train_index]\n",
    "            validation_input = image_set[validation_index]\n",
    "            \n",
    "            cnn_model = creator(NUM_CLASSES)\n",
    "            cnn_model.compile(optimizer='adam', \n",
    "                              loss='categorical_crossentropy', \n",
    "                              metrics=['accuracy'])\n",
    "            \n",
    "            trainGenerator = aug_method.flow(train_input, train_label, batch_size=BATCH_SIZE)\n",
    "            history = cnn_model.fit(trainGenerator,\n",
    "                                    steps_per_epoch=100,\n",
    "                                    epochs=NUM_EPOCH, \n",
    "                                    validation_data = (validation_input / 255.0, validation_label))\n",
    "            \n",
    "            base_train_result.append(cnn_model.predict(train_input      / 255.0))\n",
    "            base_valid_result.append(cnn_model.predict(validation_input / 255.0))\n",
    "        \n",
    "        # Combine outputs from base models\n",
    "        train_features = np.hstack(base_train_result)\n",
    "        valid_features = np.hstack(base_valid_result)\n",
    "        train_label_int = np.argmax(train_label, axis=1)\n",
    "        valid_label_int = np.argmax(validation_label, axis=1)\n",
    "        \n",
    "        # Meta-classifier\n",
    "        log_reg = LogisticRegression(multi_class='multinomial', solver='lbfgs', C=NUM_CLASSES, max_iter=1000)\n",
    "        log_reg.fit(train_features, train_label_int)\n",
    "        predictions = log_reg.predict(valid_features)\n",
    "        ground_truth = valid_label_int\n",
    "        print(\"Accuracy:\", accuracy_score(valid_label_int, predictions))\n",
    "        \n",
    "        misclassify_instance = [i for i in range(len(predictions)) if predictions[i] != valid_label_int[i]]\n",
    "        correct_instance = [i for i in range(len(predictions)) if predictions[i] == valid_label_int[i]]\n",
    "        plt.figure(figsize = (15, 15))\n",
    "        ground_truth = valid_label_int\n",
    "        cf = confusion_matrix(ground_truth, predictions)\n",
    "        df_cm = pd.DataFrame(cf, index=classes, columns=classes)\n",
    "        plt.figure(figsize = (20,20))\n",
    "        sns.heatmap(df_cm, annot=True)\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"True\")\n",
    "        plt.savefig(f\"confusion_matrix_ensemble.png\")  \n",
    "        plt.show()\n",
    "        plt.figure(figsize=(12, 12))\n",
    "        wrong_img = [] \n",
    "        for i in range(min(25, len(correct_instance))): \n",
    "            index = correct_instance[i]\n",
    "            plt.subplot(5, 5, i + 1)\n",
    "            plt.imshow(validation_image[index])\n",
    "            plt.axis('off')\n",
    "            plt.title(f\"Pred: {predictions[index]}, Real: {ground_truth[index]}\")\n",
    "            wrong_img.append([validation_image[index], ground_truth[index]])\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        # Show misclassified images (up to 25)\n",
    "        plt.figure(figsize=(12, 12))\n",
    "        wrong_img = [] \n",
    "        for i in range(min(25, len(misclassify_instance))): \n",
    "            index = misclassify_instance[i]\n",
    "            plt.subplot(5, 5, i + 1)\n",
    "            plt.imshow(validation_image[index])\n",
    "            plt.axis('off')\n",
    "            plt.title(f\"Pred: {predictions[index]}, Real: {ground_truth[index]}\")\n",
    "            wrong_img.append([validation_image[index], ground_truth[index]])\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ensemble_method_evaluate(createAlexNet1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Convolutional Neural Networks Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def train_visualize(creator):\n",
    "    train_image,  validation_image, train_label, validation_label = train_test_split(train_images, train_labels, test_size = 0.2,random_state = 42)\n",
    "    cnn_model = creator(NUM_CLASSES)\n",
    "    cnn_model.compile(optimizer='adam', \n",
    "                      loss='categorical_crossentropy', \n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "    trainGenerator = aug.flow(train_image, train_label, batch_size=BATCH_SIZE) \n",
    "    history = cnn_model.fit(trainGenerator, \n",
    "                            steps_per_epoch=100,\n",
    "                            epochs=NUM_EPOCH,      \n",
    "                            validation_data=(validation_image / 255.0, validation_label))\n",
    "\n",
    "    cnn_model.save('model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_visualize(createAlexNet1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from keras import models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "model = load_model('model.h5')\n",
    "\n",
    "img_tensor = train_images[2] / 255.0\n",
    "img_tensor = np.expand_dims(img_tensor, axis=0)\n",
    "plt.imshow(img_tensor[0])\n",
    "\n",
    "\n",
    "\n",
    "layer_outputs = [layer.output for layer in model.layers[:8]]\n",
    "activation_model = models.Model(inputs = model.layers[0].input, outputs=layer_outputs)\n",
    "\n",
    "activations = activation_model.predict(img_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "first_layer_activation = activations[0]\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.suptitle(\"First Layer Activation\", fontsize=16)\n",
    "for i in range(10):\n",
    "    plt.subplot(5, 5, i + 1)\n",
    "    plt.imshow(first_layer_activation[0, :, :, i], cmap='viridis')\n",
    "    plt.title(f\"{i}th Channel Activation\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('CNN_OUTPUT_LAYER_1.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fourth_layer_activation = activations[3]\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.suptitle(\"Fourth Layer Activation\", fontsize=16)\n",
    "for i in range(10):\n",
    "    plt.subplot(5, 5, i + 1)\n",
    "    plt.imshow(fourth_layer_activation[0, :, :, i], cmap='viridis')\n",
    "    plt.title(f\"{i}th Channel Activation\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('CNN_OUTPUT_LAYER_4.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7262209,
     "sourceId": 11582480,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
